{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON: JavaScript Object Notation\n",
    "- A list of object to be transferred on Internet: serialize(flatterning the object)->encode->received->decode->deserialize\n",
    "- Stored as plain (byte strings or utf-8 strings) text\n",
    "- Contains data type information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json.loads() and json.dumps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data_string = '[{\"b\": [2, 4], \"3.0\": \"c\", \"a\": \"A\"},34]'\n",
    "# json.loads recursively decodes a string in JSON format into equivalent python objects\n",
    "python_data = json.loads(data_string)\n",
    "data_string2 = json.dumps(python_data)\n",
    "print(type(data_string))\n",
    "print(type(python_data))\n",
    "print(type(data_string2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API: Application Programming Interface\n",
    "- A protocal containing a set of commands or functions that allow one piece of software to talk to another\n",
    "- Data from the web is often gotten through an API\n",
    "- Web APIs usually consist of two parts:\n",
    "    - request an well-formed HTTP request to a server\n",
    "    - response a response from the server, usually either an html page or a JSON object"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requests library\n",
    "- The primary mechanism for sending an API request or accessing a web server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "<class 'bytes'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(\"https://www.allrecipes.com/\")\n",
    "print(response.status_code) #status code 200: the request response cycle was successful\n",
    "print(type(response.content))\n",
    "print(type(response.content.decode('utf-8'))) #Data received from the world wide web is usually encoded in utf-8\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Map Geocoding API Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_lng(address_string,api_key):\n",
    "  response_data = ''\n",
    "  lat = ''\n",
    "  lng = ''\n",
    "  address = address_string.replace(' ','_')\n",
    "  url=\"https://maps.googleapis.com/maps/api/geocode/json?address=%s&key=%s\" % (address,api_key)\n",
    "  try:\n",
    "      response = requests.get(url)\n",
    "      if not response.status_code == 200:\n",
    "          print(\"HTTP error\",response.status_code)\n",
    "      else:\n",
    "          try:\n",
    "            # requests can automatically decode and convert a json response into a python object\n",
    "            # using response.json()\n",
    "              response_data = response.json()\n",
    "              lat = response_data['results'][0]['geometry']['location']['lat']\n",
    "              lng = response_data['results'][0]['geometry']['location']['lng']\n",
    "          except:\n",
    "              print(\"Response not in valid JSON format\")\n",
    "  except:\n",
    "      print(\"Something went wrong with requests.get\")\n",
    "\n",
    "  return (lat,lng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.8075355, -73.9625727)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key = 'AIzaSyDsVcdhWZWcd_ep1hKj821pfe1g4JGHtFY'\n",
    "get_lat_lng(\"Columbia University, New York, NY\",api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML\n",
    "- eXtensible Markup Language\n",
    "- data is stored in a tree\n",
    "- data items are \"tagged\" with named values\n",
    "- html is (loosely) similar to XML (both are based on SGML)\n",
    "- The python library  **lxml** deals with converting an xml string to python objects and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> \n",
      " <Element Bookstore at 0x2f757ccdc80> <class 'lxml.etree._Element'> \n",
      "\n",
      "\n",
      "<Bookstore>\n",
      "   <Book ISBN=\"ISBN-13:978-1599620787\" Price=\"15.23\" Weight=\"1.5\">\n",
      "      <Title>New York Deco</Title>\n",
      "      <Authors>\n",
      "         <Author Residence=\"New York City\">\n",
      "            <First_Name>Richard</First_Name>\n",
      "            <Last_Name>Berenholtz</Last_Name>\n",
      "         </Author>\n",
      "      </Authors>\n",
      "   </Book>\n",
      "   <Book ISBN=\"ISBN-13:978-1579128562\" Price=\"15.80\">\n",
      "      <Remark>\n",
      "      Five Hundred Buildings of New York and over one million other books are available for Amazon Kindle.\n",
      "      </Remark>\n",
      "      <Title>Five Hundred Buildings of New York</Title>\n",
      "      <Authors>\n",
      "         <Author Residence=\"Beijing\">\n",
      "            <First_Name>Bill</First_Name>\n",
      "            <Last_Name>Harris</Last_Name>\n",
      "         </Author>\n",
      "         <Author Residence=\"New York City\">\n",
      "            <First_Name>Jorg</First_Name>\n",
      "            <Last_Name>Brockmann</Last_Name>\n",
      "         </Author>\n",
      "      </Authors>\n",
      "   </Book>\n",
      "</Bookstore>\n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_string = \"\"\"\n",
    "<Bookstore>\n",
    "   <Book ISBN=\"ISBN-13:978-1599620787\" Price=\"15.23\" Weight=\"1.5\">\n",
    "      <Title>New York Deco</Title>\n",
    "      <Authors>\n",
    "         <Author Residence=\"New York City\">\n",
    "            <First_Name>Richard</First_Name>\n",
    "            <Last_Name>Berenholtz</Last_Name>\n",
    "         </Author>\n",
    "      </Authors>\n",
    "   </Book>\n",
    "   <Book ISBN=\"ISBN-13:978-1579128562\" Price=\"15.80\">\n",
    "      <Remark>\n",
    "      Five Hundred Buildings of New York and over one million other books are available for Amazon Kindle.\n",
    "      </Remark>\n",
    "      <Title>Five Hundred Buildings of New York</Title>\n",
    "      <Authors>\n",
    "         <Author Residence=\"Beijing\">\n",
    "            <First_Name>Bill</First_Name>\n",
    "            <Last_Name>Harris</Last_Name>\n",
    "         </Author>\n",
    "         <Author Residence=\"New York City\">\n",
    "            <First_Name>Jorg</First_Name>\n",
    "            <Last_Name>Brockmann</Last_Name>\n",
    "         </Author>\n",
    "      </Authors>\n",
    "   </Book>\n",
    "</Bookstore>\n",
    "\"\"\"\n",
    "\n",
    "from lxml import etree\n",
    "root = etree.XML(data_string)\n",
    "print(type(data_string), \"\\n\", root, type(root), \"\\n\\n\")\n",
    "\n",
    "# need pretty_print and decoding to print out XML object\n",
    "print(etree.tostring(root, pretty_print=True).decode(\"utf-8\"), \"\\n\")\n",
    "type(etree.tostring(root, pretty_print=True).decode(\"utf-8\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through XML Tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. root.iter(): loose structure of the tree, but go through every node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element Bookstore at 0x2f757ccdc80>\n",
      "<Element Book at 0x2f757cc8940>\n",
      "<Element Title at 0x2f75a9c7e80>\n",
      "<Element Authors at 0x2f758fb4380>\n",
      "<Element Author at 0x2f757cc8940>\n",
      "<Element First_Name at 0x2f75a9c7e80>\n",
      "<Element Last_Name at 0x2f758fb4380>\n",
      "<Element Book at 0x2f757cc8940>\n",
      "<Element Remark at 0x2f75a9c7e80>\n",
      "<Element Title at 0x2f758fb4380>\n",
      "<Element Authors at 0x2f757cc8940>\n",
      "<Element Author at 0x2f75a9c7e80>\n",
      "<Element First_Name at 0x2f758fb4380>\n",
      "<Element Last_Name at 0x2f757cc8940>\n",
      "<Element Author at 0x2f75a9c7e80>\n",
      "<Element First_Name at 0x2f758fb4380>\n",
      "<Element Last_Name at 0x2f757cc8940>\n"
     ]
    }
   ],
   "source": [
    "for element in root.iter():\n",
    "    print(element)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. only the children level in the subtree\n",
    "- no bookstore / book information\n",
    "- no deeper author name information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element Title at 0x2f758fb4fc0>\n",
      "<Element Authors at 0x2f75aa23c00>\n",
      "<Element Remark at 0x2f75a968480>\n",
      "<Element Title at 0x2f758fb4fc0>\n",
      "<Element Authors at 0x2f75aa23c00>\n"
     ]
    }
   ],
   "source": [
    "for child in root:\n",
    "    for thing in child:\n",
    "        print(thing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing some attributes\n",
    "1. Element.tag: access the tag name\n",
    "2. Element.text: prints the text in a leaf node\n",
    "3. Element.find('tag_name'): \n",
    "    - access the FIRST node's content with specified tag names\n",
    "    - .find function only looks at the children level, not other descendants\n",
    "4. Element.findall(): finds ALL elements with a tag which are direct children of the current element. Element.find() finds the first child with a particular tag.\n",
    "5. Element.get() accesses the element’s attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harris\n",
      "Brockmann\n",
      "Richard Berenholtz\n",
      "Jorg Brockmann\n"
     ]
    }
   ],
   "source": [
    "for element in root.findall(\"Book[@Price ='15.80']/Authors/Author/Last_Name\"): \n",
    "#use @ to add constrains on certain attribute\n",
    "    print(element.text)\n",
    "    \n",
    "for element in root.findall('Book/Authors/Author[@Residence = \"New York City\"]'):\n",
    "  print(element.find('First_Name').text,element.find('Last_Name').text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python libraries for web scraping**\n",
    "- requests for handling the request-response cycle\n",
    "- beautifulsoup4 for extracting data from an html string\n",
    "- selenium for extracting data from an html string and managing the response process, particularly when a page contains JavaScript or when a button needs to be clicked"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup\n",
    "- html and xml parser\n",
    "- makes use of formatted html tags and css properties to extract data\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. find_all():\n",
    "    - finds all instances of a specified tag\n",
    "    - returns a bs4 result_set (a list)\n",
    "2. find():\n",
    "    - finds the first instance of a specified tag\n",
    "    - returns a bs4 element\n",
    "    \n",
    "***bs4 functions can be recursively applied on bs4 elements***\n",
    "\n",
    "3. CSS selectors：used in both find_all() and find() functions\n",
    "    - selector=value\n",
    "    - dictionary: {\"keyword selector\": value}\n",
    "4. get_text(): returns the marked up text (the content, string) enclosed in a tag\n",
    "5. get(): returns the value of a tag attribute (e.g. \"href\", \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# get all recipes names and links, and descriptions\n",
    "def get_recipes(keywords):\n",
    "\n",
    "    recipe_list = list()\n",
    "    \n",
    "    url = \"https://www.allrecipes.com/search?q=\" + keywords\n",
    "    # we didn't replace space with \"_\" because\n",
    "    # .get() encode the url, so there can be space in the url\n",
    "    response = requests.get(url)\n",
    "    if not response.status_code == 200:\n",
    "        print(\"HTTP error\",response.status_code)\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        results_page = BeautifulSoup(response.content,'lxml') # parse the response into readable format\n",
    "        recipes = results_page.find_all('div',{\"class\":\"comp card-list mntl-document-card-list mntl-card-list mntl-block\"})\n",
    "        for recipe in recipes:\n",
    "            recipe_link = recipe.find('a').get('href')\n",
    "            recipe_name = recipe.find('span').get('card__title-text ')\n",
    "            try:\n",
    "                # re.compile helps to match any class attribute with the content as \"card_summary\" and any letter behind\n",
    "                recipe_description = recipe.find('div',{\"class\":re.compile('card__summary.*')}).get_text().replace(\"\\n\",'').strip()\n",
    "            except:\n",
    "                recipe_description = ''\n",
    "            recipe_list.append((recipe_name,recipe_link,recipe_description))\n",
    "        return recipe_list\n",
    "    except:\n",
    "        print(\"Something went wrong with nodes tag finding\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# get a recipe's information given a recipe's link\n",
    "def get_recipe_info(recipe_link):\n",
    "    recipe_dict = dict()\n",
    "\n",
    "    try:\n",
    "        response = requests.get(recipe_link)\n",
    "        if not response.status_code == 200:\n",
    "            print(\"HTTP error\",response.status_code)\n",
    "            return recipe_dict\n",
    "        \n",
    "        result_page = BeautifulSoup(response.content,'lxml')\n",
    "        ingredient_list = list()\n",
    "        prep_steps_list = list()\n",
    "\n",
    "        ingredientsection = result_page.find('ul',{\"class\":\"ingredients-section\"})\n",
    "        for ingredient in ingredientsection.find_all('li'):\n",
    "            ingredient_list.append(ingredient.find('span',{\"class\":re.compile('ingredients-item-name.*')}).get_text())\n",
    "\n",
    "        stepssection = result_page.find('ul',{\"class\":\"instructions-section\"})\n",
    "        for step in stepssection.find_all('li'):\n",
    "            prep_steps_list.append(step.find('p').get_text())\n",
    "\n",
    "        recipe_dict['ingredients'] = ingredient_list\n",
    "        recipe_dict['preparation'] = prep_steps_list\n",
    "        return recipe_dict\n",
    "        \n",
    "    except:\n",
    "        print(\"Something went wrong with request.get\")\n",
    "        return recipe_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the above two functions\n",
    "# \n",
    "def get_all_recipes(keywords):\n",
    "    results = list()\n",
    "    all_recipes = get_recipes(keywords) # return all recipe_name, recipe_link, recipe_description\n",
    "    for recipe in all_recipes:\n",
    "        recipe_dict = get_recipe_info(recipe[1]) # get each information in the recipe_link extracted\n",
    "        recipe_dict['name'] = recipe[0]\n",
    "        recipe_dict['description'] = recipe[2]\n",
    "        results.append(recipe_dict)\n",
    "    return(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Authentication\n",
    "Login while Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 page on your watchlist (excluding talk pages). Changes to pages since you last visited them are shown in bold with solid markers. \n"
     ]
    }
   ],
   "source": [
    "username = 'VickyYu7'\n",
    "password = 'Yuweiqi1314288'\n",
    "\n",
    "# Construct an object that contains the data to be sent to the login page\n",
    "payload = {\n",
    "    'wpName': username,\n",
    "    'wpPassword': password,\n",
    "#     'wploginattempt': 'Log in',\n",
    "#     'wpEditToken': \"+\\\\\",\n",
    "#     'title': \"Special:UserLogin\",\n",
    "    'authAction': \"login\",\n",
    "    }\n",
    "# get the value of the login token\n",
    "def get_login_token(response):\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    token = soup.find('input',{'name':\"wpLoginToken\"}).get('value') # value is the attribute of the \"name\" tag\n",
    "    return token\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with requests.session() as s: # create a new session object\n",
    "    response = s.get('https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Main+Page') # use a session to get the response\n",
    "                                                      # Server sends back the login session\n",
    "    payload['wpLoginToken'] = get_login_token(response) # use username and password to get the login token\n",
    "    # post(url, login_data)\n",
    "    # Send the login request, send Server the data\n",
    "    # If data is not the Server want, it will ignore it\n",
    "    response_post = s.post('https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Main+Page',\n",
    "                           data=payload)\n",
    "    #Get another page and check if we’re still logged in, Server controls the time of each session\n",
    "    #You can use session.close() to manually close the session\n",
    "    response = s.get('https://en.wikipedia.org/wiki/Special:Watchlist')\n",
    "    data = BeautifulSoup(response.content,'lxml')\n",
    "    print(data.find('div',class_='watchlistDetails').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2ad4952f491032d71626255937cf2715f7899e712ce34d34f8ad00db942d78f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
